{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are the LLMs playing like a human? \n",
    "Goal: check if the LLMs have a behaviour similar to humans on Wikispeedia.\n",
    "\n",
    "Strategy: make a LLM play to 2007 Wikispeedia and compare its answers with human paths.\n",
    "\n",
    "Steps for a MWE:\n",
    "1. decide which path to use (take one played a lot by people to have more data to compare with)\n",
    "2. take a LLM\n",
    "3. define the prompt\n",
    "4. make it play\n",
    "5. compare with paths of humans\n",
    "\n",
    "Points to adjust:\n",
    "- try different paths\n",
    "- try different LLMs\n",
    "- try different prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model too busy, unable to get response in less than 60 second(s)'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"how are you doing?\",\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4220c0ed4ad4037ae5ec2afc358f442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = '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'\n",
    "\n",
    "# Replace with the model ID you want to download\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# Download and cache the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "save_directory = \"./local_models/Llama-3.2-3B\"  # Specify your local path here\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./local_models/bert-large-cased-whole-word-masking-finetuned-squad/tokenizer_config.json',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/special_tokens_map.json',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/vocab.txt',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/added_tokens.json',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = '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'\n",
    "\n",
    "# Replace with the model ID you want to download\n",
    "model_name = \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "save_directory = \"./local_models/bert-large-cased-whole-word-masking-finetuned-squad\"  # Specify your local path here\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Download and cache the model\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "model_identifier must be a string or LLM object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@lmql\u001b[39m\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     17\u001b[0m                 model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m                 \u001b[38;5;66;03m# model=lmql.model(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@lmql\u001b[39m\u001b[38;5;241m.\u001b[39mquery\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchain_of_thought\u001b[39m(question):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''lmql\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    # Q&A prompt template\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"Q: {question}\\n\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    return ANSWER\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(chain_of_thought(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToday is the 12th of June, what day was it 1 week ago?\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/api/queries.py:148\u001b[0m, in \u001b[0;36mquery.<locals>.lmql_query_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fct)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlmql_query_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/lmql_runtime.py:204\u001b[0m, in \u001b[0;36mLMQLQueryFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_async:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m call_sync(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__acall__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/loop.py:37\u001b[0m, in \u001b[0;36mcall_sync\u001b[0;34m(lmql_query_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     res \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# try nested event loop\u001b[39;00m\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/lmql_runtime.py:230\u001b[0m, in \u001b[0;36mLMQLQueryFunction.__acall__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m=\u001b[39m interpreter\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# execute main prompt\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfct, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquery_kwargs)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m==\u001b[39m interpreter:\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/tracing/tracer.py:240\u001b[0m, in \u001b[0;36mtrace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     tracer \u001b[38;5;241m=\u001b[39m Tracer(name)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextTracer(tracer):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fct(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/interpreter.py:955\u001b[0m, in \u001b[0;36mPromptInterpreter.run\u001b[0;34m(self, fct, *args, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m query_head \u001b[38;5;241m=\u001b[39m InterpretationHead(fct, context, args, kwargs)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_state \u001b[38;5;241m=\u001b[39m PromptState(interpreter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, subinterpreters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    948\u001b[0m     variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, stmt_buffer\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    949\u001b[0m     query_head\u001b[38;5;241m=\u001b[39mquery_head, program_state\u001b[38;5;241m=\u001b[39mcontext\u001b[38;5;241m.\u001b[39mprogram_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    953\u001b[0m     stopping_phrases\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    954\u001b[0m     tail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_state)\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdebug_out\u001b[39m(decoder_step):\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/interpreter.py:385\u001b[0m, in \u001b[0;36mPromptInterpreter.advance\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m query_head\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stmt_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m continue_for_more_prompt_stmts()\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m distribution_reached:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stmt_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror: distribution variable must be the last statement in a prompt, but found \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(format_buffer())\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/interpreter.py:365\u001b[0m, in \u001b[0;36mPromptInterpreter.advance.<locals>.continue_for_more_prompt_stmts\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m query_head\u001b[38;5;241m.\u001b[39mfresh_copy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery head must be fresh copy to avoid state sharing side effects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m     query_head\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m LMQLContext(\u001b[38;5;28mself\u001b[39m, state, prompt)\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m query_head\u001b[38;5;241m.\u001b[39mcontinue_()\n\u001b[1;32m    367\u001b[0m qstring \u001b[38;5;241m=\u001b[39m query_head\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    368\u001b[0m query_args_after_last_continue \u001b[38;5;241m=\u001b[39m query_head\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_head\u001b[38;5;241m.\u001b[39mcurrent_args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/multi_head_interpretation.py:140\u001b[0m, in \u001b[0;36mInterpretationHead.continue_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator_fct()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_current_arg()\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/multi_head_interpretation.py:112\u001b[0m, in \u001b[0;36mInterpretationHead.handle_current_arg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fct(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(res)\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupt:\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/multi_head_interpretation.py:89\u001b[0m, in \u001b[0;36mInterpretationHead.advance\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator_fct()\u001b[38;5;241m.\u001b[39masend(result)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_current_arg()\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/multi_head_interpretation.py:111\u001b[0m, in \u001b[0;36mInterpretationHead.handle_current_arg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fct(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_args[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(res)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/interpreter.py:166\u001b[0m, in \u001b[0;36mLMQLContext.set_model\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpreter\u001b[38;5;241m.\u001b[39mset_model(model_name)\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/runtime/interpreter.py:319\u001b[0m, in \u001b[0;36mPromptInterpreter.set_model\u001b[0;34m(self, model_handle)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     model_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m--> 319\u001b[0m model_handle \u001b[38;5;241m=\u001b[39m LLM\u001b[38;5;241m.\u001b[39mfrom_descriptor(model_handle)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_identifier \u001b[38;5;241m=\u001b[39m model_handle\u001b[38;5;241m.\u001b[39mmodel_identifier\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# setup the VocabularyMatcher to use the concrete vocabulary of the model\u001b[39;00m\n",
      "File \u001b[0;32m/bin/anaconda3/envs/ada/lib/python3.11/site-packages/lmql/api/llm.py:167\u001b[0m, in \u001b[0;36mLLM.from_descriptor\u001b[0;34m(cls, model_identifier, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_identifier \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<dynamic>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m model_identifier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     model_identifier \u001b[38;5;241m=\u001b[39m get_default_model()\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_identifier, (\u001b[38;5;28mstr\u001b[39m, LLM)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_identifier must be a string or LLM object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# check for user-defined shorthands\u001b[39;00m\n\u001b[1;32m    170\u001b[0m model_identifier \u001b[38;5;241m=\u001b[39m resolve_user_shorthands(\u001b[38;5;28mcls\u001b[39m, model_identifier)\n",
      "\u001b[0;31mAssertionError\u001b[0m: model_identifier must be a string or LLM object"
     ]
    }
   ],
   "source": [
    "import lmql \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# lmql.serve(\"Llama-2-13b-hf\", cuda=True, port=9999, trust_remote_code=True)\n",
    "\n",
    "# Specify the path where the model and tokenizer are saved\n",
    "load_directory = \"./local_models/bert-large-cased-whole-word-masking-finetuned-squad\"  # Your local path\n",
    "\n",
    "# Load the model and tokenizer from the specified directory\n",
    "model = AutoModelForCausalLM.from_pretrained(load_directory, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "\n",
    "@lmql.query(\n",
    "                model=model,\n",
    "                # model=lmql.model(\n",
    "                #     \"local_models/bert-large-cased-whole-word-masking-finetuned-squad\", endpoint=\"localhost:2830\", trust_env = True, trust_remote_code=True, #10.91.44.122\n",
    "                # ),\n",
    "                decoder=\"sample\",\n",
    "                temperature=0.5,\n",
    "                top_k=10,\n",
    "                max_len=4096\n",
    "            )\n",
    "# lmql.model(model_name)\n",
    "@lmql.query\n",
    "def chain_of_thought(question):\n",
    "    '''lmql\n",
    "    # Q&A prompt template\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A: Let's think step by step.\\n\"\n",
    "    \"[REASONING]\"\n",
    "    \"Thus, the answer is:[ANSWER].\"\n",
    "\n",
    "    # return just the ANSWER to the caller\n",
    "    return ANSWER\n",
    "    '''\n",
    "\n",
    "print(chain_of_thought('Today is the 12th of June, what day was it 1 week ago?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e13748f53a4a3cbb4ac7d804a11e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '5+5=? (5+5)\\n5+5=? (5+5)\\n5'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"5+5=?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths to use to try the LLMs\n",
    "Let's find the most finished paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hashedIpAddress</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>durationInSec</th>\n",
       "      <th>path</th>\n",
       "      <th>rating</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>%E2%82%AC2_commemorative_coins</th>\n",
       "      <th>Irish_Sea</th>\n",
       "      <th>32997</th>\n",
       "      <td>651ff0fa4fac4471</td>\n",
       "      <td>1227628729</td>\n",
       "      <td>15</td>\n",
       "      <td>[%E2%82%AC2_commemorative_coins, Ireland, Iris...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>%E2%82%AC2_commemorative_coins</td>\n",
       "      <td>Irish_Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">10th_century</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">11th_century</th>\n",
       "      <th>29758</th>\n",
       "      <td>516b61133d358ce1</td>\n",
       "      <td>1224623308</td>\n",
       "      <td>6</td>\n",
       "      <td>[10th_century, 11th_century]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>11th_century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29759</th>\n",
       "      <td>6b039e9953cf075e</td>\n",
       "      <td>1241187124</td>\n",
       "      <td>3</td>\n",
       "      <td>[10th_century, 11th_century]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>11th_century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29760</th>\n",
       "      <td>0aecf97906bcb41a</td>\n",
       "      <td>1373439892</td>\n",
       "      <td>4</td>\n",
       "      <td>[10th_century, 11th_century]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>11th_century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banknote</th>\n",
       "      <th>29761</th>\n",
       "      <td>32652d6d1c5d9351</td>\n",
       "      <td>1260397548</td>\n",
       "      <td>48</td>\n",
       "      <td>[10th_century, Maya_civilization, Silver, Coin...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>Banknote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Zulu</th>\n",
       "      <th>Doom</th>\n",
       "      <th>50811</th>\n",
       "      <td>267f588369bcec64</td>\n",
       "      <td>1248704813</td>\n",
       "      <td>455</td>\n",
       "      <td>[Zulu, AK-47, Canada, &lt;, &lt;, English_language, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Doom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jesus</th>\n",
       "      <th>50807</th>\n",
       "      <td>027433ec4cc9ff72</td>\n",
       "      <td>1368505587</td>\n",
       "      <td>7</td>\n",
       "      <td>[Zulu, Christianity, Jesus]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Jesus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th>50806</th>\n",
       "      <td>4ee1908c061cfc53</td>\n",
       "      <td>1260241074</td>\n",
       "      <td>29</td>\n",
       "      <td>[Zulu, English_language, Language]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Slovenia</th>\n",
       "      <th>50812</th>\n",
       "      <td>401e0e507140865e</td>\n",
       "      <td>1249233736</td>\n",
       "      <td>56</td>\n",
       "      <td>[Zulu, South_Africa, Continent, Europe, Slovenia]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Slovenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50813</th>\n",
       "      <td>767f706b73fa806a</td>\n",
       "      <td>1265042003</td>\n",
       "      <td>63</td>\n",
       "      <td>[Zulu, South_Africa, Mediterranean_Sea, Slovenia]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Slovenia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51318 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hashedIpAddress  \\\n",
       "start                          end                                    \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  651ff0fa4fac4471   \n",
       "10th_century                   11th_century 29758  516b61133d358ce1   \n",
       "                                            29759  6b039e9953cf075e   \n",
       "                                            29760  0aecf97906bcb41a   \n",
       "                               Banknote     29761  32652d6d1c5d9351   \n",
       "...                                                             ...   \n",
       "Zulu                           Doom         50811  267f588369bcec64   \n",
       "                               Jesus        50807  027433ec4cc9ff72   \n",
       "                               Language     50806  4ee1908c061cfc53   \n",
       "                               Slovenia     50812  401e0e507140865e   \n",
       "                                            50813  767f706b73fa806a   \n",
       "\n",
       "                                                    timestamp  durationInSec  \\\n",
       "start                          end                                             \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  1227628729             15   \n",
       "10th_century                   11th_century 29758  1224623308              6   \n",
       "                                            29759  1241187124              3   \n",
       "                                            29760  1373439892              4   \n",
       "                               Banknote     29761  1260397548             48   \n",
       "...                                                       ...            ...   \n",
       "Zulu                           Doom         50811  1248704813            455   \n",
       "                               Jesus        50807  1368505587              7   \n",
       "                               Language     50806  1260241074             29   \n",
       "                               Slovenia     50812  1249233736             56   \n",
       "                                            50813  1265042003             63   \n",
       "\n",
       "                                                                                                path  \\\n",
       "start                          end                                                                     \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  [%E2%82%AC2_commemorative_coins, Ireland, Iris...   \n",
       "10th_century                   11th_century 29758                       [10th_century, 11th_century]   \n",
       "                                            29759                       [10th_century, 11th_century]   \n",
       "                                            29760                       [10th_century, 11th_century]   \n",
       "                               Banknote     29761  [10th_century, Maya_civilization, Silver, Coin...   \n",
       "...                                                                                              ...   \n",
       "Zulu                           Doom         50811  [Zulu, AK-47, Canada, <, <, English_language, ...   \n",
       "                               Jesus        50807                        [Zulu, Christianity, Jesus]   \n",
       "                               Language     50806                 [Zulu, English_language, Language]   \n",
       "                               Slovenia     50812  [Zulu, South_Africa, Continent, Europe, Slovenia]   \n",
       "                                            50813  [Zulu, South_Africa, Mediterranean_Sea, Slovenia]   \n",
       "\n",
       "                                                   rating  \\\n",
       "start                          end                          \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997     1.0   \n",
       "10th_century                   11th_century 29758     1.0   \n",
       "                                            29759     5.0   \n",
       "                                            29760     1.0   \n",
       "                               Banknote     29761     3.0   \n",
       "...                                                   ...   \n",
       "Zulu                           Doom         50811     3.0   \n",
       "                               Jesus        50807     NaN   \n",
       "                               Language     50806     NaN   \n",
       "                               Slovenia     50812     1.0   \n",
       "                                            50813     1.0   \n",
       "\n",
       "                                                                            start  \\\n",
       "start                          end                                                  \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  %E2%82%AC2_commemorative_coins   \n",
       "10th_century                   11th_century 29758                    10th_century   \n",
       "                                            29759                    10th_century   \n",
       "                                            29760                    10th_century   \n",
       "                               Banknote     29761                    10th_century   \n",
       "...                                                                           ...   \n",
       "Zulu                           Doom         50811                            Zulu   \n",
       "                               Jesus        50807                            Zulu   \n",
       "                               Language     50806                            Zulu   \n",
       "                               Slovenia     50812                            Zulu   \n",
       "                                            50813                            Zulu   \n",
       "\n",
       "                                                            end  \n",
       "start                          end                               \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997     Irish_Sea  \n",
       "10th_century                   11th_century 29758  11th_century  \n",
       "                                            29759  11th_century  \n",
       "                                            29760  11th_century  \n",
       "                               Banknote     29761      Banknote  \n",
       "...                                                         ...  \n",
       "Zulu                           Doom         50811          Doom  \n",
       "                               Jesus        50807         Jesus  \n",
       "                               Language     50806      Language  \n",
       "                               Slovenia     50812      Slovenia  \n",
       "                                            50813      Slovenia  \n",
       "\n",
       "[51318 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "DATA_PATH = 'data/wikispeedia_paths-and-graph/'\n",
    "\n",
    "# load tsv files into pandas dataframes\n",
    "path_finished = pd.read_csv(os.path.join(DATA_PATH, 'paths_finished.tsv'), sep='\\t', comment='#', names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'])\n",
    "\n",
    "path_finished.path = path_finished.path.str.split(';')\n",
    "path_finished['start'] = path_finished.path.str[0]\n",
    "path_finished['end'] = path_finished.path.str[-1]\n",
    "\n",
    "by_path = path_finished.groupby(by=['start', 'end']).apply(lambda x: x)\n",
    "by_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start                     end              \n",
       "Asteroid                  Viking               1043\n",
       "Brain                     Telephone            1040\n",
       "Theatre                   Zebra                 905\n",
       "Pyramid                   Bean                  642\n",
       "Batman                    Wood                  148\n",
       "Bird                      Great_white_shark     138\n",
       "Batman                    The_Holocaust         119\n",
       "Bird                      Adolf_Hitler          107\n",
       "Beer                      Sun                    99\n",
       "Batman                    Banana                 69\n",
       "Cat                       Computer               57\n",
       "                          Microsoft              56\n",
       "Dog                       Telephone              53\n",
       "Flower                    Adolf_Hitler           51\n",
       "Automobile                Pluto                  47\n",
       "Dog                       Venus                  47\n",
       "Batman                    Bible                  43\n",
       "Aluminium_chloride        Parrot                 42\n",
       "England                   God                    42\n",
       "Achilles_tendon           Ocean                  40\n",
       "Calculus                  Paul_McCartney         39\n",
       "Aircraft                  Google                 35\n",
       "Electricity               God                    34\n",
       "Art                       Mango                  32\n",
       "Brazil                    Hydrogen               32\n",
       "Africa                    England                30\n",
       "Archbishop_of_Canterbury  Vietnam                30\n",
       "Computer                  Russia                 29\n",
       "                          Fruit                  29\n",
       "Cat                       Adolf_Hitler           27\n",
       "14th_century              Rainbow                27\n",
       "Jesus                     God                    27\n",
       "Manchester                Water                  26\n",
       "China                     Moon                   25\n",
       "Antlion                   Hip_hop_music          24\n",
       "Batman                    Vitamin_D              24\n",
       "                          Penguin                24\n",
       "Jesus                     Adolf_Hitler           24\n",
       "Batman                    Japan                  24\n",
       "Brothers_Grimm            Windows_Vista          23\n",
       "Achilles_tendon           Ivory                  23\n",
       "Batman                    Albert_Einstein        23\n",
       "Calculus                  Lemon                  21\n",
       "California                Adolf_Hitler           21\n",
       "Computer_programming      Fresh_water            21\n",
       "Apple                     Banana                 20\n",
       "Automobile                Windows_XP             20\n",
       "Cat                       Dog                    20\n",
       "God                       Adolf_Hitler           19\n",
       "Moon                      Mars                   19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "by_path_counts = path_finished.groupby(by=['start', 'end']).size().sort_values(ascending=False)\n",
    "by_path_counts[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas of prompt:\n",
    "\n",
    "- from AI vs human project:\n",
    "`We now play the following game:\n",
    "\n",
    "I will give you a target word and a list from which you can choose an option. If the list contains the target word, you choose it. Otherwise you choose the option that is most similar to it. Before starting, I give you one examples, then it's your turn:\n",
    "\n",
    "EXAMPLE:\n",
    "Target word: George_Washington\n",
    "\n",
    "Available options: [[Able_Archer_83, Afghanistan, , Estonia, Europe, Finland, France, French_language, George_W._Bush, Hungary, September_11,_2001_attacks, United_States]]\n",
    "\n",
    "Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States.\n",
    "\n",
    "Answer: Hence the answer is: 'United_States'.\n",
    "\n",
    "YOUR TURN:\n",
    "\n",
    "Target word: {TARGET}\n",
    "\n",
    "Available options: [[{LIST OF LINKS}]]\n",
    "\n",
    "Reasoning: [REASONING]\n",
    "\n",
    "Answer: Hence the choice is: '[ANSWER]'`\n",
    "\n",
    "\n",
    "- ordering the options by order of appearence in text, to reproduce the fact that this is what humans see first.\n",
    "`We now play the following game:\n",
    "\n",
    "I will give you a target word and a list (in no specific order) from which you can choose an option. If the list contains the target word, you choose it. Otherwise you choose the option that is most similar to it. Before starting, I give you one examples, then it's your turn:\n",
    "\n",
    "EXAMPLE:\n",
    "Target word: George_Washington\n",
    "\n",
    "Available options: [change]\n",
    "\n",
    "Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States.\n",
    "\n",
    "Answer: Hence the answer is: 'United_States'.\n",
    "\n",
    "YOUR TURN:\n",
    "\n",
    "Target word: {TARGET}\n",
    "\n",
    "Available options: [[{LIST OF LINKS}]]\n",
    "\n",
    "Reasoning: [REASONING]\n",
    "\n",
    "Answer: Hence the choice is: '[ANSWER]'`\n",
    "\n",
    "\n",
    "\n",
    "- explicit further the game \n",
    "`We now play the following game: I give you a target encyclopedia article, the current article you are in and the list of other article titles present in this current article. The final goal is to reach the target article as fast as possible by navigating from article to article. Your task right now is to choose an article from the list I give you that brings you closer to the target. If the target is present in the list, return the target.\n",
    "Before starting, I give you one examples, then it's your turn:\n",
    "\n",
    "EXAMPLE:\n",
    "Target word: George_Washington\n",
    "\n",
    "Available options: [change]\n",
    "\n",
    "Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States.\n",
    "\n",
    "Answer: Hence the answer is: 'United_States'.\n",
    "\n",
    "YOUR TURN:\n",
    "\n",
    "Target word: {TARGET}\n",
    "\n",
    "Available options: [[{LIST OF LINKS}]]\n",
    "\n",
    "Reasoning: [REASONING]\n",
    "\n",
    "Answer: Hence the choice is: '[ANSWER]'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
