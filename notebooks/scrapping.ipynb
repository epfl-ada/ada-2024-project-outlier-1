{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from time import sleep\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eglantinevialaneix/Desktop/ADA/Project/ada-2024-project-outlier-1/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__file__ = os.getcwd()\n",
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.dirname(__file__)))\n",
    "\n",
    "# Import the module\n",
    "import src.scripts.scrapper_and_writters as scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.path.dirname(__file__), 'data/2007/')\n",
    "articles = pd.read_csv(os.path.join(DATA_PATH, 'articles.tsv'), sep='\\t', comment='#', names=['article'])\n",
    "urls = articles['article'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping articles' links from the articles names of Wikispeedia 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:   8%|▊         | 375/4604 [04:31<55:49,  1.26article/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Athletics_%28track_and_field%29: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  13%|█▎        | 588/4604 [07:17<53:14,  1.26article/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Bionicle__Mask_of_Light: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  26%|██▋       | 1211/4604 [15:13<40:52,  1.38article/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Directdebit: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  35%|███▍      | 1601/4604 [20:10<38:57,  1.28article/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Friend_Directdebit: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  35%|███▌      | 1628/4604 [20:25<32:53,  1.51article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Gallery_of_the_Kings_and_Queens_of_England: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  65%|██████▍   | 2970/4604 [37:35<21:53,  1.24article/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Newshounds: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  84%|████████▎ | 3850/4604 [48:22<08:08,  1.54article/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Sponsorship_Directdebit: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  84%|████████▍ | 3879/4604 [48:45<12:01,  1.00article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Star_Wars_Episode_IV__A_New_Hope: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  97%|█████████▋| 4481/4604 [56:15<01:47,  1.14article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Wikipedia_Text_of_the_GNU_Free_Documentation_License: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  99%|█████████▊| 4546/4604 [56:59<00:38,  1.49article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping Wowpurchase: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles:  99%|█████████▉| 4553/4604 [57:04<00:35,  1.45article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping X-Men__The_Last_Stand: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles: 100%|██████████| 4604/4604 [57:40<00:00,  1.33article/s]\n"
     ]
    }
   ],
   "source": [
    "article_links, article_names = scr.scrape_wikipedia_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate links in 2024\n",
    "article_links = pd.DataFrame.from_dict(article_links).drop_duplicates()\n",
    "article_names = pd.DataFrame.from_dict(article_names).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr.export_links_to_csv(article_links, \"data/2024/raw_links2024.csv\")\n",
    "scr.export_articles_to_csv(article_names, \"data/2024/raw_articles2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv(os.path.join(DATA_PATH, 'links.tsv'), sep='\\t', comment='#', names=['linkSource', 'linkTarget'])\n",
    "\n",
    "# compare the number of links by source target in links2024.csv and links.tsv\n",
    "links2024 = pd.read_csv('data/2024/raw_links2024.csv')\n",
    "\n",
    "# filter to get the same linkSource in links that are in links2024\n",
    "links = links[links['linkSource'].isin(links2024['linkSource'])]\n",
    "\n",
    "# Count the number of ouput links by source target\n",
    "links_count = links.groupby(['linkSource']).size().reset_index(name='count')\n",
    "links2024_count = links2024.groupby(['linkSource']).size().reset_index(name='count')\n",
    "\n",
    "# compare the number of links by source target in links2024.csv and links.tsv\n",
    "comparison = np.abs(links_count['count'] - links2024_count['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in number of links by article in total between 2007 and now (subset of 500 articles): 251.97%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    4535.000000\n",
       "mean       66.196913\n",
       "std        88.924844\n",
       "min         0.000000\n",
       "25%        12.000000\n",
       "50%        33.000000\n",
       "75%        79.000000\n",
       "max      1374.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Difference in number of links by article in total between 2007 and now (subset of 500 articles): {comparison.sum() / links.shape[0] * 100:.2f}%\")\n",
    "comparison.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377149, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links2024.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4604, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-scrapping links from 2024 articles that changed name since 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the list of article names to scrap their links from wikipedia\n",
    "\n",
    "# Starting with the exact same list as in Wikispeedia 2007\n",
    "actual_article_names_2024 = articles[\"article_2007\"]\n",
    "\n",
    "\n",
    "# TODO: use replace instead of this kk:\n",
    "\n",
    "# First we update the name of the seven articles we want to keep\n",
    "# What was their name in 2007\n",
    "old_unmatched_names = [\"Athletics_%28track_and_field%29\",\n",
    "                       \"Bionicle__Mask_of_Light\", \n",
    "                       \"Directdebit\",\n",
    "                       \"Newshounds\",\n",
    "                       \"Star_Wars_Episode_IV__A_New_Hope\",\n",
    "                       \"Wikipedia_Text_of_the_GNU_Free_Documentation_License\",\n",
    "                       \"X-Men__The_Last_Stand\"]\n",
    "# Where were they in the data\n",
    "unmatched_index = [i for i, article in enumerate(articles['article_2007']) if article in old_unmatched_names]\n",
    "# What are the articles names now\n",
    "new_names = [\"Track_and_field\",\n",
    "             \"Bionicle:_Mask_of_Light\",\n",
    "             \"Direct_debit\",\n",
    "             \"News_Hounds\",\n",
    "             \"Star_Wars_(film)\",\n",
    "             \"Wikipedia:Text_of_the_GNU_Free_Documentation_License\",\n",
    "             \"X-Men:_The_Last_Stand\"]\n",
    "# Update de list of article names with the seven new names\n",
    "for i, new_name in enumerate(new_names):\n",
    "    actual_article_names_2024[unmatched_index[i]] = new_name\n",
    "\n",
    "# We remove the four articles that do not have an equivalent in 2024\n",
    "missing_articles_names = [\"Friend_Directdebit\", \"Gallery_of_the_Kings_and_Queens_of_England\", \"Sponsorship_Directdebit\", \"Wowpurchase\"]\n",
    "missing_index = [i for i, article in enumerate(articles['article_2007']) if article in missing_articles_names]\n",
    "actual_article_names_2024 = actual_article_names_2024.drop(missing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying which articles to scrap\n",
    "urls = actual_article_names_2024.tolist()\n",
    "\n",
    "# Scrapping\n",
    "missing_articles_links, _ = scr.scrape_wikipedia_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating the names of all new names to the old names\n",
    "missing_articles_links = pd.DataFrame.from_dict(missing_articles_links)\n",
    "missing_articles_links = missing_articles_links.replace(to_replace = new_names, value = old_unmatched_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate links in 2024\n",
    "missing_articles_links = missing_articles_links.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr.export_df_links_to_csv(missing_articles_links, \"data/2024/links2024.csv\")\n",
    "scr.export_articles_to_csv(articles[\"article_2007\"], \"data/2024/articles2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping back disambiguation pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 43\n"
     ]
    }
   ],
   "source": [
    "deleted_articles = [\"Friend_Directdebit\",\n",
    "                    \"Gallery_of_the_Kings_and_Queens_of_England\",\n",
    "                    \"Sponsorship_Directdebit\",\n",
    "                    \"Wowpurchase\"]\n",
    "\n",
    "old_unmatched_names = [\"Athletics_%28track_and_field%29\",\n",
    "                       \"Bionicle__Mask_of_Light\", \n",
    "                       \"Directdebit\",\n",
    "                       \"Newshounds\",\n",
    "                       \"Star_Wars_Episode_IV__A_New_Hope\",\n",
    "                       \"Wikipedia_Text_of_the_GNU_Free_Documentation_License\",\n",
    "                       \"X-Men__The_Last_Stand\"]\n",
    "\n",
    "new_names = [\"Track_and_field\",\n",
    "             \"Bionicle:_Mask_of_Light\",\n",
    "             \"Direct_debit\",\n",
    "             \"News_Hounds\",\n",
    "             \"Star_Wars_(film)\",\n",
    "             \"Wikipedia:Text_of_the_GNU_Free_Documentation_License\",\n",
    "             \"X-Men:_The_Last_Stand\"]\n",
    "\n",
    "old_ambiguous_names = [\"Aggregator\",\n",
    "                       \"Anne_of_Great_Britain\",\n",
    "                       \"Bantu\",\n",
    "                       \"Battle_of_Amiens\",\n",
    "                       \"Blackbird\",\n",
    "                       \"Bj%C3%B8rn%C3%B8ya\",\n",
    "                       \"Boa\",\n",
    "                       \"Boston_RFC\",\n",
    "                       \"Brabantian\",\n",
    "                       \"Dark_Ages\",\n",
    "                       \"David_Heymann\",\n",
    "                       \"Defaka\",\n",
    "                       \"Doom\",\n",
    "                       \"Firecrest\",\n",
    "                       \"Forth\",\n",
    "                       \"Garage_%28dance_music%29\",\n",
    "                       \"Herring_Gull\",\n",
    "                       \"Industry\",\n",
    "                       \"Lake_Albert\",\n",
    "                       \"Mark_Webber\",\n",
    "                       \"Market\",\n",
    "                       \"Nagorno-Karabakh_War\",\n",
    "                       \"Newmarket\",\n",
    "                       \"Pochard\",\n",
    "                       \"Prehistoric_man\",\n",
    "                       \"Recorder\",\n",
    "                       \"Red_Panda\",\n",
    "                       \"Sandur\",\n",
    "                       \"Scent_of_a_Woman\",\n",
    "                       \"Sequoia\",\n",
    "                       \"Serenity_%28film%29\",\n",
    "                       \"Sparrowhawk\",\n",
    "                       \"Swift\",\n",
    "                       \"Terik\",\n",
    "                       \"Tooth_development\",\n",
    "                       \"Tripoli\",\n",
    "                       \"Underground_%28stories%29\",\n",
    "                       \"Weymouth\",\n",
    "                       \"Whitethroat\",\n",
    "                       \"William_Gilbert\",\n",
    "                       \"Winfield_Scott_%28ship%29\",\n",
    "                       \"Woodruff\",\n",
    "                       \"Zulu\"]\n",
    "\n",
    "new_disambiguous_names = [\"News_aggregator\",\n",
    "                        \"Anne,_Queen_of_Great_Britain\",\n",
    "                        \"Bantu_peoples\",\n",
    "                        \"Battle_of_Amiens_(1918)\",\n",
    "                        \"Common_blackbird\",\n",
    "                        \"Bear_Island_(Svalbard)\",\n",
    "                        \"Boa_(genus)\",\n",
    "                        \"Boston_RFC_(United_States)\",\n",
    "                        \"Brabantian_Dutch\",\n",
    "                        \"Dark_Ages_(historiography)\",\n",
    "                        \"David_Heymann_(architect)\",\n",
    "                        \"Defaka_people\",\n",
    "                        \"Doom_(1993_video_game)\",\n",
    "                        \"Common_firecrest\",\n",
    "                        \"Forth_(programming_language)\",\n",
    "                        \"Garage_house\",\n",
    "                        \"American_herring_gull\",\n",
    "                        \"Industry_(economics)\",\n",
    "                        \"Lake_Albert_(Africa)\",\n",
    "                        \"Mark_Webber_(racing_driver)\",\n",
    "                        \"Market_(economics)\",\n",
    "                        \"First_Nagorno-Karabakh_War\",\n",
    "                        \"Newmarket,_Suffolk\",\n",
    "                        \"Common_pochard\",\n",
    "                        \"Prehistory\",\n",
    "                        \"Recorder_(musical_instrument)\",\n",
    "                        \"Red_panda\",\n",
    "                        \"Outwash_plain\",\n",
    "                        \"Scent_of_a_Woman_(1992_film)\",\n",
    "                        \"Sequoia_(genus)\",\n",
    "                        \"Serenity_(2005_film)\",\n",
    "                        \"Eurasian_sparrowhawk\",\n",
    "                        \"Swift_(bird)\",\n",
    "                        \"Terik_people\",\n",
    "                        \"Human_tooth_development\",\n",
    "                        \"Tripoli,_Libya\",\n",
    "                        \"Underground_(Murakami_book)\",\n",
    "                        \"Weymouth,_Dorset\",\n",
    "                        \"Common_whitethroat\",\n",
    "                        \"William_Gilbert_(physicist)\",\n",
    "                        \"SS_Winfield_Scott\",\n",
    "                        \"Galium_odoratum\",\n",
    "                        \"Zulu_people\"]\n",
    "\n",
    "print(len(old_ambiguous_names), len(new_disambiguous_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%C3%85land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%C3%89douard_Manet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>%C3%89ire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>%C3%93engus_I_of_the_Picts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>Zionism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>Zirconium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>Zoroaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>Zuid-Gelders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>Zulu_people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               article\n",
       "0     %C3%81ed%C3%A1n_mac_Gabr%C3%A1in\n",
       "1                           %C3%85land\n",
       "2                   %C3%89douard_Manet\n",
       "3                            %C3%89ire\n",
       "4           %C3%93engus_I_of_the_Picts\n",
       "...                                ...\n",
       "4599                           Zionism\n",
       "4600                         Zirconium\n",
       "4601                         Zoroaster\n",
       "4602                      Zuid-Gelders\n",
       "4603                       Zulu_people\n",
       "\n",
       "[4600 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_names = articles.replace(old_ambiguous_names, new_disambiguous_names)\n",
    "updated_names = updated_names.replace(old_unmatched_names, new_names)\n",
    "\n",
    "# We remove the four articles that do not have an equivalent in 2024\n",
    "missing_index = [i for i, article in enumerate(articles['article']) if article in deleted_articles]\n",
    "actual_article_names_2024 = updated_names.drop(missing_index)\n",
    "\n",
    "actual_article_names_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Wikipedia articles: 100%|██████████| 4600/4600 [27:12<00:00,  2.82article/s]\n"
     ]
    }
   ],
   "source": [
    "urls = actual_article_names_2024.article.tolist()\n",
    "\n",
    "# Scraping\n",
    "# TODO rescrape with changed name for article names variable\n",
    "disamb_articles_links, _ = scr.scrape_wikipedia_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dictionnary of the links into a list\n",
    "list_disamb_articles_links = []\n",
    "for key in disamb_articles_links.keys():\n",
    "    for value in disamb_articles_links[key]:\n",
    "        list_disamb_articles_links.append([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating the names of all new names to the old names\n",
    "df_disamb_articles_links = pd.DataFrame(list_disamb_articles_links)\n",
    "df_disamb_articles_links = df_disamb_articles_links.replace(to_replace = new_disambiguous_names, value = old_ambiguous_names)\n",
    "df_disamb_articles_links = df_disamb_articles_links.replace(to_replace = new_names, value = old_unmatched_names)\n",
    "\n",
    "# Removing duplicate links in 2024\n",
    "df_disamb_articles_links = df_disamb_articles_links.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr.export_df_links_to_csv(df_disamb_articles_links, \"../data/2024/links2024.csv\")\n",
    "scr.export_articles_to_csv(_, \"../data/2024/articles2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping article categories\n",
    "idea: write a function to retriev only article names and categories to find which articles became \"desambiguous pages\" in 2024 and thus have almost no useful link that redirect the player to an article from the 4604 of the database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
