{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from time import sleep\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = os.getcwd()\n",
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the Python path\n",
    "os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), '')))\n",
    "         \n",
    "# Import the module\n",
    "import utils.scrapper_and_writters as scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '../data/2007/'))\n",
    "articles = pd.read_csv(os.path.join(DATA_PATH, 'articles.tsv'), sep='\\t', comment='#', names=['article'])\n",
    "urls = articles['article'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping articles' links from the articles names of Wikispeedia 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: running the scrapping parts will take at least 30 minutes for each scrapping. Meaning a total of at least 1h30min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links, article_names = scr.scrape_wikipedia_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate links in 2024\n",
    "article_links = pd.DataFrame.from_dict(article_links).drop_duplicates()\n",
    "article_names = pd.DataFrame.from_dict(article_names).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporting = False\n",
    "if exporting:\n",
    "    scr.export_df_links_to_csv(article_links, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../data/2024/raw_links2024.csv\")))\n",
    "    scr.export_articles_to_csv(article_names, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../data/2024/raw_articles2024.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv(os.path.join(DATA_PATH, 'links.tsv'), sep='\\t', comment='#', names=['linkSource', 'linkTarget'])\n",
    "\n",
    "# compare the number of links by source target in links2024.csv and links.tsv\n",
    "links2024 = pd.read_csv('data/2024/raw_links2024.csv')\n",
    "\n",
    "# filter to get the same linkSource in links that are in links2024\n",
    "links = links[links['linkSource'].isin(links2024['linkSource'])]\n",
    "\n",
    "# Count the number of ouput links by source target\n",
    "links_count = links.groupby(['linkSource']).size().reset_index(name='count')\n",
    "links2024_count = links2024.groupby(['linkSource']).size().reset_index(name='count')\n",
    "\n",
    "# compare the number of links by source target in links2024.csv and links.tsv\n",
    "comparison = np.abs(links_count['count'] - links2024_count['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Difference in number of links by article in total between 2007 and now (subset of 500 articles): {comparison.sum() / links.shape[0] * 100:.2f}%\")\n",
    "comparison.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links2024.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-scrapping links from 2024 articles that changed name since 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the list of article names to scrap their links from wikipedia\n",
    "\n",
    "# Starting with the exact same list as in Wikispeedia 2007\n",
    "actual_article_names_2024 = articles[\"article_2007\"]\n",
    "\n",
    "\n",
    "# First we update the name of the seven articles we want to keep\n",
    "# What was their name in 2007\n",
    "old_unmatched_names = [\"Athletics_%28track_and_field%29\",\n",
    "                       \"Bionicle__Mask_of_Light\", \n",
    "                       \"Directdebit\",\n",
    "                       \"Newshounds\",\n",
    "                       \"Star_Wars_Episode_IV__A_New_Hope\",\n",
    "                       \"Wikipedia_Text_of_the_GNU_Free_Documentation_License\",\n",
    "                       \"X-Men__The_Last_Stand\"]\n",
    "# Where were they in the data\n",
    "unmatched_index = [i for i, article in enumerate(articles['article_2007']) if article in old_unmatched_names]\n",
    "# What are the articles names now\n",
    "new_names = [\"Track_and_field\",\n",
    "             \"Bionicle:_Mask_of_Light\",\n",
    "             \"Direct_debit\",\n",
    "             \"News_Hounds\",\n",
    "             \"Star_Wars_(film)\",\n",
    "             \"Wikipedia:Text_of_the_GNU_Free_Documentation_License\",\n",
    "             \"X-Men:_The_Last_Stand\"]\n",
    "# Update de list of article names with the seven new names\n",
    "for i, new_name in enumerate(new_names):\n",
    "    actual_article_names_2024[unmatched_index[i]] = new_name\n",
    "\n",
    "# We remove the four articles that do not have an equivalent in 2024\n",
    "missing_articles_names = [\"Friend_Directdebit\", \"Gallery_of_the_Kings_and_Queens_of_England\", \"Sponsorship_Directdebit\", \"Wowpurchase\"]\n",
    "missing_index = [i for i, article in enumerate(articles['article_2007']) if article in missing_articles_names]\n",
    "actual_article_names_2024 = actual_article_names_2024.drop(missing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying which articles to scrap\n",
    "urls = actual_article_names_2024.tolist()\n",
    "\n",
    "# Scrapping\n",
    "missing_articles_links, _ = scr.scrape_wikipedia_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating the names of all new names to the old names\n",
    "missing_articles_links = pd.DataFrame.from_dict(missing_articles_links)\n",
    "missing_articles_links = missing_articles_links.replace(to_replace = new_names, value = old_unmatched_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate links in 2024\n",
    "missing_articles_links = missing_articles_links.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporting = False\n",
    "if exporting:\n",
    "    scr.export_df_links_to_csv(missing_articles_links, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../data/2024/raw_links2024.csv\")))\n",
    "    scr.export_articles_to_csv(articles[\"article_2007\"], os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../data/2024/raw_articles2024.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping back disambiguation pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_articles = [\"Friend_Directdebit\",\n",
    "                    \"Gallery_of_the_Kings_and_Queens_of_England\",\n",
    "                    \"Sponsorship_Directdebit\",\n",
    "                    \"Wowpurchase\"]\n",
    "\n",
    "old_unmatched_names = [\"Athletics_%28track_and_field%29\",\n",
    "                       \"Bionicle__Mask_of_Light\", \n",
    "                       \"Directdebit\",\n",
    "                       \"Newshounds\",\n",
    "                       \"Star_Wars_Episode_IV__A_New_Hope\",\n",
    "                       \"Wikipedia_Text_of_the_GNU_Free_Documentation_License\",\n",
    "                       \"X-Men__The_Last_Stand\"]\n",
    "\n",
    "new_names = [\"Track_and_field\",\n",
    "             \"Bionicle:_Mask_of_Light\",\n",
    "             \"Direct_debit\",\n",
    "             \"News_Hounds\",\n",
    "             \"Star_Wars_(film)\",\n",
    "             \"Wikipedia:Text_of_the_GNU_Free_Documentation_License\",\n",
    "             \"X-Men:_The_Last_Stand\"]\n",
    "\n",
    "old_ambiguous_names = [\"Aggregator\",\n",
    "                       \"Anne_of_Great_Britain\",\n",
    "                       \"Bantu\",\n",
    "                       \"Battle_of_Amiens\",\n",
    "                       \"Blackbird\",\n",
    "                       \"Bj%C3%B8rn%C3%B8ya\",\n",
    "                       \"Boa\",\n",
    "                       \"Boston_RFC\",\n",
    "                       \"Brabantian\",\n",
    "                       \"Dark_Ages\",\n",
    "                       \"David_Heymann\",\n",
    "                       \"Defaka\",\n",
    "                       \"Doom\",\n",
    "                       \"Firecrest\",\n",
    "                       \"Forth\",\n",
    "                       \"Garage_%28dance_music%29\",\n",
    "                       \"Herring_Gull\",\n",
    "                       \"Industry\",\n",
    "                       \"Lake_Albert\",\n",
    "                       \"Mark_Webber\",\n",
    "                       \"Market\",\n",
    "                       \"Nagorno-Karabakh_War\",\n",
    "                       \"Newmarket\",\n",
    "                       \"Pochard\",\n",
    "                       \"Prehistoric_man\",\n",
    "                       \"Recorder\",\n",
    "                       \"Red_Panda\",\n",
    "                       \"Sandur\",\n",
    "                       \"Scent_of_a_Woman\",\n",
    "                       \"Sequoia\",\n",
    "                       \"Serenity_%28film%29\",\n",
    "                       \"Sparrowhawk\",\n",
    "                       \"Swift\",\n",
    "                       \"Terik\",\n",
    "                       \"Tooth_development\",\n",
    "                       \"Tripoli\",\n",
    "                       \"Underground_%28stories%29\",\n",
    "                       \"Weymouth\",\n",
    "                       \"Whitethroat\",\n",
    "                       \"William_Gilbert\",\n",
    "                       \"Winfield_Scott_%28ship%29\",\n",
    "                       \"Woodruff\",\n",
    "                       \"Zulu\"]\n",
    "\n",
    "new_disambiguous_names = [\"News_aggregator\",\n",
    "                        \"Anne,_Queen_of_Great_Britain\",\n",
    "                        \"Bantu_peoples\",\n",
    "                        \"Battle_of_Amiens_(1918)\",\n",
    "                        \"Common_blackbird\",\n",
    "                        \"Bear_Island_(Svalbard)\",\n",
    "                        \"Boa_(genus)\",\n",
    "                        \"Boston_RFC_(United_States)\",\n",
    "                        \"Brabantian_Dutch\",\n",
    "                        \"Dark_Ages_(historiography)\",\n",
    "                        \"David_Heymann_(architect)\",\n",
    "                        \"Defaka_people\",\n",
    "                        \"Doom_(1993_video_game)\",\n",
    "                        \"Common_firecrest\",\n",
    "                        \"Forth_(programming_language)\",\n",
    "                        \"Garage_house\",\n",
    "                        \"American_herring_gull\",\n",
    "                        \"Industry_(economics)\",\n",
    "                        \"Lake_Albert_(Africa)\",\n",
    "                        \"Mark_Webber_(racing_driver)\",\n",
    "                        \"Market_(economics)\",\n",
    "                        \"First_Nagorno-Karabakh_War\",\n",
    "                        \"Newmarket,_Suffolk\",\n",
    "                        \"Common_pochard\",\n",
    "                        \"Prehistory\",\n",
    "                        \"Recorder_(musical_instrument)\",\n",
    "                        \"Red_panda\",\n",
    "                        \"Outwash_plain\",\n",
    "                        \"Scent_of_a_Woman_(1992_film)\",\n",
    "                        \"Sequoia_(genus)\",\n",
    "                        \"Serenity_(2005_film)\",\n",
    "                        \"Eurasian_sparrowhawk\",\n",
    "                        \"Swift_(bird)\",\n",
    "                        \"Terik_people\",\n",
    "                        \"Human_tooth_development\",\n",
    "                        \"Tripoli,_Libya\",\n",
    "                        \"Underground_(Murakami_book)\",\n",
    "                        \"Weymouth,_Dorset\",\n",
    "                        \"Common_whitethroat\",\n",
    "                        \"William_Gilbert_(physicist)\",\n",
    "                        \"SS_Winfield_Scott\",\n",
    "                        \"Galium_odoratum\",\n",
    "                        \"Zulu_people\"]\n",
    "\n",
    "print(len(old_ambiguous_names), len(new_disambiguous_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_names = articles.replace(old_ambiguous_names, new_disambiguous_names)\n",
    "updated_names = updated_names.replace(old_unmatched_names, new_names)\n",
    "\n",
    "# We remove the four articles that do not have an equivalent in 2024\n",
    "missing_index = [i for i, article in enumerate(articles['article']) if article in deleted_articles]\n",
    "actual_article_names_2024 = updated_names.drop(missing_index)\n",
    "\n",
    "actual_article_names_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = actual_article_names_2024.article.tolist()\n",
    "\n",
    "# Scraping\n",
    "# TODO rescrape with changed name for article names variable\n",
    "disamb_articles_links, disamb_names = scr.scrape_wikipedia_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dictionnary of the links into a list\n",
    "list_disamb_articles_links = []\n",
    "for key in disamb_articles_links.keys():\n",
    "    for value in disamb_articles_links[key]:\n",
    "        list_disamb_articles_links.append([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating the names of all new names to the old names\n",
    "df_disamb_articles_links = pd.DataFrame(list_disamb_articles_links)\n",
    "df_disamb_articles_links = df_disamb_articles_links.replace(to_replace = new_disambiguous_names, value = old_ambiguous_names)\n",
    "df_disamb_articles_links = df_disamb_articles_links.replace(to_replace = new_names, value = old_unmatched_names)\n",
    "\n",
    "# Removing duplicate links in 2024\n",
    "df_disamb_articles_links = df_disamb_articles_links.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporting = False\n",
    "if exporting:\n",
    "    scr.export_df_links_to_csv(df_disamb_articles_links, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../data/2024/raw_links2024.csv\")))\n",
    "    scr.export_articles_to_csv(disamb_names, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../data/2024/raw_articles2024.csv\")))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
