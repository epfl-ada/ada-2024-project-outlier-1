{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are the LLMs playing like a human? \n",
    "Goal: check if the LLMs have a behaviour similar to humans on Wikispeedia.\n",
    "\n",
    "Strategy: make a LLM play to 2007 Wikispeedia and compare its answers with human paths.\n",
    "\n",
    "Steps for a MWE:\n",
    "1. decide which path to use (take one played a lot by people to have more data to compare with)\n",
    "2. take a LLM\n",
    "3. define the prompt\n",
    "4. make it play\n",
    "5. compare with paths of humans\n",
    "\n",
    "Points to adjust:\n",
    "- try different paths\n",
    "- try different LLMs\n",
    "- try different prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'how are you doing? how are your life friends and families? and how is the decision,for getting new laptop/pc? Hope you are using a note taking software to take notes and you are well done using it, if not please click on the below pic of ME and then on the button “note taking” which takes you to the appropraite page.Hope the time didnot go past so quickly. But I don’t even unerstand how and where is the time going.\\nHey, guys.. time'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"how are you doing?\",\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e2bf09f4334425be1f17726977cd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = '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'\n",
    "\n",
    "# Replace with the model ID you want to download\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Download and cache the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "save_directory = \"./local_models/meta-llama/Llama-3.1-8B-Instruct\"  # Specify your local path here\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcb078742df463d95a9bf4d772ab176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0161e0be237b43eeb1c90c013c44d913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at google-bert/bert-large-cased-whole-word-masking-finetuned-squad and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65be2896c0e24943a3e908e38bfc2d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08eaed2a9204eeb93a04dd412f1f653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88b55c1115b42eb923d1b6441c460e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./local_models/bert-large-cased-whole-word-masking-finetuned-squad/tokenizer_config.json',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/special_tokens_map.json',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/vocab.txt',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/added_tokens.json',\n",
       " './local_models/bert-large-cased-whole-word-masking-finetuned-squad/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = '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'\n",
    "\n",
    "# Replace with the model ID you want to download\n",
    "model_name = \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "save_directory = \"./local_models/bert-large-cased-whole-word-masking-finetuned-squad\"  # Specify your local path here\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, cache_dir=\"./local_models\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./local_models\")\n",
    "\n",
    "# Download and cache the model\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3088ccb2dd10428ea64b8915de3e1f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = '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'\n",
    "\n",
    "# Replace with the model ID you want to download\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "save_directory = \"./local_models/mistralai/Mistral-7B-Instruct-v0.3\"  # Specify your local path here\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, cache_dir=\"./local_models\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./local_models\")\n",
    "\n",
    "# Download and cache the model\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hf_hub_download' from 'transformers' (/bin/anaconda3/envs/ada/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Specify the model and download location\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteknium/OpenHermes-2.5-Mistral-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'hf_hub_download' from 'transformers' (/bin/anaconda3/envs/ada/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import hf_hub_download\n",
    "\n",
    "# Specify the model and download location\n",
    "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "save_directory = \"./local_models/teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# List of files to download\n",
    "files = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"config.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"vocab.txt\"\n",
    "]\n",
    "\n",
    "# Download each file individually\n",
    "for filename in files:\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    try:\n",
    "        hf_hub_download(repo_id=model_name, filename=filename, cache_dir=save_directory)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
      "[Loading ./local_models/meta-llama/openai-community/gpt2 with AutoModelForCausalLM.from_pretrained(\"./local_models/meta-llama/openai-community/gpt2\", trust_remote_code=True)]]\n",
      "[Serving LMTP endpoint on ws://localhost:9999/]\n",
      "[./local_models/meta-llama/openai-community/gpt2 ready on device cpu]\n"
     ]
    },
    {
     "ename": "GracefulExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mGracefulExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/anaconda3/envs/ada/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import lmql \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# lmql.serve(\"Llama-2-13b-hf\", cuda=True, port=9999, trust_remote_code=True)\n",
    "\n",
    "# Specify the path where the model and tokenizer are saved\n",
    "load_directory = \"./local_models/meta-llama/openai-community/gpt2\"  # Your local path\n",
    "\n",
    "# Load the model and tokenizer from the specified directory\n",
    "model = AutoModel.from_pretrained(load_directory, trust_remote_code=True)\n",
    "print(type(model))\n",
    "@lmql.query(\n",
    "                    lmql.serve(load_directory, port=9999, trust_remote_code=True),\n",
    "                    model = AutoModel.from_pretrained(load_directory, trust_remote_code=True, cache_dir=load_directory),\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(load_directory, cache_dir=load_directory)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "@lmql.query\n",
    "def chain_of_thought(question):\n",
    "    print('coucou')\n",
    "    '''lmql\n",
    "    # Q&A prompt template\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A: Let's think step by step.\\n\"\n",
    "    \"[REASONING]\"\n",
    "    \"Thus, the answer is:[ANSWER].\"\n",
    "\n",
    "    # return just the ANSWER to the caller\n",
    "    return ANSWER\n",
    "    '''\n",
    "\n",
    "print(chain_of_thought('Today is the 12th of June, what day was it 1 week ago?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serving LMTP endpoint on ws://localhost:8080/]\n"
     ]
    }
   ],
   "source": [
    "import lmql \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# lmql.serve(\"Llama-2-13b-hf\", cuda=True, port=9999, trust_remote_code=True)\n",
    "\n",
    "# Specify the path where the model and tokenizer are saved\n",
    "load_directory = \"./local_models/meta-llama/openai-community/gpt2\"  # Your local path\n",
    "\n",
    "# Load the model and tokenizer from the specified directory\n",
    "# model = AutoModel.from_pretrained(load_directory, trust_remote_code=True)\n",
    "# print(type(model))\n",
    "                    \n",
    "                \n",
    "                \n",
    "lmql.serve(load_directory, port=8080, trust_remote_code=True)\n",
    "@lmql.query(\n",
    "                    model=load_directory\n",
    ")\n",
    "def chain_of_thought(question):\n",
    "    '''lmql\n",
    "    # Q&A prompt template\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A: Let's think step by step.\\n\"\n",
    "    \"[REASONING]\"\n",
    "    \"Thus, the answer is:[ANSWER].\"\n",
    "\n",
    "    # return just the ANSWER to the caller\n",
    "    return ANSWER\n",
    "    '''\n",
    "\n",
    "print(chain_of_thought('Today is the 12th of June, what day was it 1 week ago?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a3f6b0870448cc85d2d346a5f7f52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/bin/anaconda3/envs/ada/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '5+5=? How to get 10\\nThe answer to 5+5 is '}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"5+5=?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --shm-size 1g --ulimit memlock=-1 --ulimit stack=67108864 -p 2830:80 \\\n",
    "    -v /home/julia/files/EPFLph/MA3/ADA/project/wikispeedia/local_models/bert-large-cased-whole-word-masking-finetuned-squad:/data ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths to use to try the LLMs\n",
    "Let's find the most finished paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hashedIpAddress</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>durationInSec</th>\n",
       "      <th>path</th>\n",
       "      <th>rating</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>%E2%82%AC2_commemorative_coins</th>\n",
       "      <th>Irish_Sea</th>\n",
       "      <th>32997</th>\n",
       "      <td>651ff0fa4fac4471</td>\n",
       "      <td>1227628729</td>\n",
       "      <td>15</td>\n",
       "      <td>[%E2%82%AC2_commemorative_coins, Ireland, Iris...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>%E2%82%AC2_commemorative_coins</td>\n",
       "      <td>Irish_Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">10th_century</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">11th_century</th>\n",
       "      <th>29758</th>\n",
       "      <td>516b61133d358ce1</td>\n",
       "      <td>1224623308</td>\n",
       "      <td>6</td>\n",
       "      <td>[10th_century, 11th_century]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>11th_century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29759</th>\n",
       "      <td>6b039e9953cf075e</td>\n",
       "      <td>1241187124</td>\n",
       "      <td>3</td>\n",
       "      <td>[10th_century, 11th_century]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>11th_century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29760</th>\n",
       "      <td>0aecf97906bcb41a</td>\n",
       "      <td>1373439892</td>\n",
       "      <td>4</td>\n",
       "      <td>[10th_century, 11th_century]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>11th_century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banknote</th>\n",
       "      <th>29761</th>\n",
       "      <td>32652d6d1c5d9351</td>\n",
       "      <td>1260397548</td>\n",
       "      <td>48</td>\n",
       "      <td>[10th_century, Maya_civilization, Silver, Coin...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10th_century</td>\n",
       "      <td>Banknote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Zulu</th>\n",
       "      <th>Doom</th>\n",
       "      <th>50811</th>\n",
       "      <td>267f588369bcec64</td>\n",
       "      <td>1248704813</td>\n",
       "      <td>455</td>\n",
       "      <td>[Zulu, AK-47, Canada, &lt;, &lt;, English_language, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Doom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jesus</th>\n",
       "      <th>50807</th>\n",
       "      <td>027433ec4cc9ff72</td>\n",
       "      <td>1368505587</td>\n",
       "      <td>7</td>\n",
       "      <td>[Zulu, Christianity, Jesus]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Jesus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th>50806</th>\n",
       "      <td>4ee1908c061cfc53</td>\n",
       "      <td>1260241074</td>\n",
       "      <td>29</td>\n",
       "      <td>[Zulu, English_language, Language]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Slovenia</th>\n",
       "      <th>50812</th>\n",
       "      <td>401e0e507140865e</td>\n",
       "      <td>1249233736</td>\n",
       "      <td>56</td>\n",
       "      <td>[Zulu, South_Africa, Continent, Europe, Slovenia]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Slovenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50813</th>\n",
       "      <td>767f706b73fa806a</td>\n",
       "      <td>1265042003</td>\n",
       "      <td>63</td>\n",
       "      <td>[Zulu, South_Africa, Mediterranean_Sea, Slovenia]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zulu</td>\n",
       "      <td>Slovenia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51318 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hashedIpAddress  \\\n",
       "start                          end                                    \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  651ff0fa4fac4471   \n",
       "10th_century                   11th_century 29758  516b61133d358ce1   \n",
       "                                            29759  6b039e9953cf075e   \n",
       "                                            29760  0aecf97906bcb41a   \n",
       "                               Banknote     29761  32652d6d1c5d9351   \n",
       "...                                                             ...   \n",
       "Zulu                           Doom         50811  267f588369bcec64   \n",
       "                               Jesus        50807  027433ec4cc9ff72   \n",
       "                               Language     50806  4ee1908c061cfc53   \n",
       "                               Slovenia     50812  401e0e507140865e   \n",
       "                                            50813  767f706b73fa806a   \n",
       "\n",
       "                                                    timestamp  durationInSec  \\\n",
       "start                          end                                             \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  1227628729             15   \n",
       "10th_century                   11th_century 29758  1224623308              6   \n",
       "                                            29759  1241187124              3   \n",
       "                                            29760  1373439892              4   \n",
       "                               Banknote     29761  1260397548             48   \n",
       "...                                                       ...            ...   \n",
       "Zulu                           Doom         50811  1248704813            455   \n",
       "                               Jesus        50807  1368505587              7   \n",
       "                               Language     50806  1260241074             29   \n",
       "                               Slovenia     50812  1249233736             56   \n",
       "                                            50813  1265042003             63   \n",
       "\n",
       "                                                                                                path  \\\n",
       "start                          end                                                                     \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  [%E2%82%AC2_commemorative_coins, Ireland, Iris...   \n",
       "10th_century                   11th_century 29758                       [10th_century, 11th_century]   \n",
       "                                            29759                       [10th_century, 11th_century]   \n",
       "                                            29760                       [10th_century, 11th_century]   \n",
       "                               Banknote     29761  [10th_century, Maya_civilization, Silver, Coin...   \n",
       "...                                                                                              ...   \n",
       "Zulu                           Doom         50811  [Zulu, AK-47, Canada, <, <, English_language, ...   \n",
       "                               Jesus        50807                        [Zulu, Christianity, Jesus]   \n",
       "                               Language     50806                 [Zulu, English_language, Language]   \n",
       "                               Slovenia     50812  [Zulu, South_Africa, Continent, Europe, Slovenia]   \n",
       "                                            50813  [Zulu, South_Africa, Mediterranean_Sea, Slovenia]   \n",
       "\n",
       "                                                   rating  \\\n",
       "start                          end                          \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997     1.0   \n",
       "10th_century                   11th_century 29758     1.0   \n",
       "                                            29759     5.0   \n",
       "                                            29760     1.0   \n",
       "                               Banknote     29761     3.0   \n",
       "...                                                   ...   \n",
       "Zulu                           Doom         50811     3.0   \n",
       "                               Jesus        50807     NaN   \n",
       "                               Language     50806     NaN   \n",
       "                               Slovenia     50812     1.0   \n",
       "                                            50813     1.0   \n",
       "\n",
       "                                                                            start  \\\n",
       "start                          end                                                  \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997  %E2%82%AC2_commemorative_coins   \n",
       "10th_century                   11th_century 29758                    10th_century   \n",
       "                                            29759                    10th_century   \n",
       "                                            29760                    10th_century   \n",
       "                               Banknote     29761                    10th_century   \n",
       "...                                                                           ...   \n",
       "Zulu                           Doom         50811                            Zulu   \n",
       "                               Jesus        50807                            Zulu   \n",
       "                               Language     50806                            Zulu   \n",
       "                               Slovenia     50812                            Zulu   \n",
       "                                            50813                            Zulu   \n",
       "\n",
       "                                                            end  \n",
       "start                          end                               \n",
       "%E2%82%AC2_commemorative_coins Irish_Sea    32997     Irish_Sea  \n",
       "10th_century                   11th_century 29758  11th_century  \n",
       "                                            29759  11th_century  \n",
       "                                            29760  11th_century  \n",
       "                               Banknote     29761      Banknote  \n",
       "...                                                         ...  \n",
       "Zulu                           Doom         50811          Doom  \n",
       "                               Jesus        50807         Jesus  \n",
       "                               Language     50806      Language  \n",
       "                               Slovenia     50812      Slovenia  \n",
       "                                            50813      Slovenia  \n",
       "\n",
       "[51318 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "DATA_PATH = 'data/wikispeedia_paths-and-graph/'\n",
    "\n",
    "# load tsv files into pandas dataframes\n",
    "path_finished = pd.read_csv(os.path.join(DATA_PATH, 'paths_finished.tsv'), sep='\\t', comment='#', names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'])\n",
    "\n",
    "path_finished.path = path_finished.path.str.split(';')\n",
    "path_finished['start'] = path_finished.path.str[0]\n",
    "path_finished['end'] = path_finished.path.str[-1]\n",
    "\n",
    "by_path = path_finished.groupby(by=['start', 'end']).apply(lambda x: x)\n",
    "by_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start                     end              \n",
       "Asteroid                  Viking               1043\n",
       "Brain                     Telephone            1040\n",
       "Theatre                   Zebra                 905\n",
       "Pyramid                   Bean                  642\n",
       "Batman                    Wood                  148\n",
       "Bird                      Great_white_shark     138\n",
       "Batman                    The_Holocaust         119\n",
       "Bird                      Adolf_Hitler          107\n",
       "Beer                      Sun                    99\n",
       "Batman                    Banana                 69\n",
       "Cat                       Computer               57\n",
       "                          Microsoft              56\n",
       "Dog                       Telephone              53\n",
       "Flower                    Adolf_Hitler           51\n",
       "Automobile                Pluto                  47\n",
       "Dog                       Venus                  47\n",
       "Batman                    Bible                  43\n",
       "Aluminium_chloride        Parrot                 42\n",
       "England                   God                    42\n",
       "Achilles_tendon           Ocean                  40\n",
       "Calculus                  Paul_McCartney         39\n",
       "Aircraft                  Google                 35\n",
       "Electricity               God                    34\n",
       "Art                       Mango                  32\n",
       "Brazil                    Hydrogen               32\n",
       "Africa                    England                30\n",
       "Archbishop_of_Canterbury  Vietnam                30\n",
       "Computer                  Russia                 29\n",
       "                          Fruit                  29\n",
       "Cat                       Adolf_Hitler           27\n",
       "14th_century              Rainbow                27\n",
       "Jesus                     God                    27\n",
       "Manchester                Water                  26\n",
       "China                     Moon                   25\n",
       "Antlion                   Hip_hop_music          24\n",
       "Batman                    Vitamin_D              24\n",
       "                          Penguin                24\n",
       "Jesus                     Adolf_Hitler           24\n",
       "Batman                    Japan                  24\n",
       "Brothers_Grimm            Windows_Vista          23\n",
       "Achilles_tendon           Ivory                  23\n",
       "Batman                    Albert_Einstein        23\n",
       "Calculus                  Lemon                  21\n",
       "California                Adolf_Hitler           21\n",
       "Computer_programming      Fresh_water            21\n",
       "Apple                     Banana                 20\n",
       "Automobile                Windows_XP             20\n",
       "Cat                       Dog                    20\n",
       "God                       Adolf_Hitler           19\n",
       "Moon                      Mars                   19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "by_path_counts = path_finished.groupby(by=['start', 'end']).size().sort_values(ascending=False)\n",
    "by_path_counts[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas of prompt:\n",
    "\n",
    "- from AI vs human project:\n",
    "`We now play the following game:\n",
    "\n",
    "I will give you a target word and a list from which you can choose an option. If the list contains the target word, you choose it. Otherwise you choose the option that is most similar to it. Before starting, I give you one examples, then it's your turn:\n",
    "\n",
    "EXAMPLE:\n",
    "Target word: George_Washington\n",
    "\n",
    "Available options: [[Able_Archer_83, Afghanistan, , Estonia, Europe, Finland, France, French_language, George_W._Bush, Hungary, September_11,_2001_attacks, United_States]]\n",
    "\n",
    "Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States.\n",
    "\n",
    "Answer: Hence the answer is: 'United_States'.\n",
    "\n",
    "YOUR TURN:\n",
    "\n",
    "Target word: {TARGET}\n",
    "\n",
    "Available options: [[{LIST OF LINKS}]]\n",
    "\n",
    "Reasoning: [REASONING]\n",
    "\n",
    "Answer: Hence the choice is: '[ANSWER]'`\n",
    "\n",
    "\n",
    "- ordering the options by order of appearence in text, to reproduce the fact that this is what humans see first.\n",
    "`We now play the following game:\n",
    "\n",
    "I will give you a target word and a list (in no specific order) from which you can choose an option. If the list contains the target word, you choose it. Otherwise you choose the option that is most similar to it. Before starting, I give you one examples, then it's your turn:\n",
    "\n",
    "EXAMPLE:\n",
    "Target word: George_Washington\n",
    "\n",
    "Available options: [change]\n",
    "\n",
    "Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States.\n",
    "\n",
    "Answer: Hence the answer is: 'United_States'.\n",
    "\n",
    "YOUR TURN:\n",
    "\n",
    "Target word: {TARGET}\n",
    "\n",
    "Available options: [[{LIST OF LINKS}]]\n",
    "\n",
    "Reasoning: [REASONING]\n",
    "\n",
    "Answer: Hence the choice is: '[ANSWER]'`\n",
    "\n",
    "\n",
    "\n",
    "- explicit further the game \n",
    "`We now play the following game: I give you a target encyclopedia article, the current article you are in and the list of other article titles present in this current article. The final goal is to reach the target article as fast as possible by navigating from article to article. Your task right now is to choose an article from the list I give you that brings you closer to the target. If the target is present in the list, return the target.\n",
    "Before starting, I give you one examples, then it's your turn:\n",
    "\n",
    "EXAMPLE:\n",
    "Target word: George_Washington\n",
    "\n",
    "Available options: [change]\n",
    "\n",
    "Reasoning: I need to find something inside the list related to the target: 'George_Washington'. George Washington was the first president of United States and he lived in United States.\n",
    "\n",
    "Answer: Hence the answer is: 'United_States'.\n",
    "\n",
    "YOUR TURN:\n",
    "\n",
    "Target word: {TARGET}\n",
    "\n",
    "Available options: [[{LIST OF LINKS}]]\n",
    "\n",
    "Reasoning: [REASONING]\n",
    "\n",
    "Answer: Hence the choice is: '[ANSWER]'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please say hello world! Eldersspace soundtrack Walden climax Walden Walden Walden Walden Walden Walden Waldenwitz Waldençaise Walden Walden Waldenwitzpreewitznapwitznapçaisenapido Waldenwitzpreewitz rink Waldenwitzpreewitz rinkwitzçaisepiesggy Waldenwitzpreewitz rinkçaise Soundtrack Walden Walden Walden Walden Walden Walden Walden Walden Walden Walden Waldenwitz Soundtrack Waldenwitznap rink Waldenwitznap rink Waldenwitznapggywitz rink Walden Walden Waldenwitznapggy Waldenwitz rink Waldenwitznapiouslyggy Waldenwitznapiously\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "load_directory = \"./local_models/bert-large-cased-whole-word-masking-finetuned-squad\"  # Your local path\n",
    "model = AutoModelForCausalLM.from_pretrained(load_directory, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "\n",
    "# Move model to CUDA explicitly\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Perform a simple inference\n",
    "inputs = tokenizer(\"please say hello world!\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs.input_ids, max_length=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the LMQL server.\n",
      "Connection closed with code: 1000 and message: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import websocket\n",
    "import json\n",
    "\n",
    "# Define the WebSocket connection\n",
    "ws = websocket.WebSocketApp(\"ws://localhost:8079\")\n",
    "\n",
    "def on_open(ws):\n",
    "    print(\"Connected to the LMQL server.\")\n",
    "    # Send a simple query directly as a string\n",
    "    ws.send(\"Q: What is the capital of France?\\nA: Let's think step by step.\\n[REASONING] Thus, the answer is:[ANSWER]\")\n",
    "\n",
    "\n",
    "def on_message(ws, message):\n",
    "    print(\"Received message:\", message)\n",
    "\n",
    "def on_error(ws, error):\n",
    "    print(\"Error:\", error)\n",
    "\n",
    "# Modify on_close to accept 3 parameters\n",
    "def on_close(ws, close_status_code, close_msg):\n",
    "    print(\"Connection closed with code:\", close_status_code, \"and message:\", close_msg)\n",
    "\n",
    "# Set up WebSocket event handlers\n",
    "ws.on_open = on_open\n",
    "ws.on_message = on_message\n",
    "ws.on_error = on_error\n",
    "ws.on_close = on_close\n",
    "\n",
    "# Run WebSocket forever to maintain connection\n",
    "ws.run_forever()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
